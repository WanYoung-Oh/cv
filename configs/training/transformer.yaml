# Transformer 모델용 학습 설정
# Vision Transformer, Swin Transformer 등에 최적화되었습니다.

learning_rate: 5e-4  # CNN보다 낮은 LR
batch_size: 16       # 메모리 제약으로 더 작음
epochs: 50
num_workers: 4

# 옵티마이저
optimizer: adamw
weight_decay: 1e-5

# 스케줄러
warmup_epochs: 10    # Transformer는 warm-up이 중요합니다 (CNN: 5)
warmup_start_factor: 0.01  # Warmup 시작 factor
min_learning_rate: 1e-6
scheduler_eta_min: 1e-6    # CosineAnnealing 최소 lr (= min_learning_rate)

# 데이터 로더
drop_last: true
pin_memory: true

# 조기 종료
early_stopping:
  monitor: val_f1
  patience: 15
  mode: max

# 체크포인트
checkpoint:
  monitor: val_f1
  mode: max
  save_top_k: 3

# 로깅
log_interval: 20

# 검증
val_check_interval: 0.5

# Mixed Precision (메모리 절약)
use_amp: true
