# 기본 학습 설정

# 하이퍼파라미터
learning_rate: 1e-3
weight_decay: 0.05
epochs: 50
batch_size: 32
num_workers: 4
# accumulate_grad_batches: 2 # 물리적 배치를 줄인 만큼 누적하여 안정성 확보

# 옵티마이저
optimizer: "adamw"

# 스케줄러
scheduler: "cosine"
warmup_epochs: 5
warmup_start_factor: 0.01  # Warmup 시작 factor (0.01 = lr의 1%부터 시작)
scheduler_eta_min: 1e-6    # CosineAnnealing 최소 lr

# 데이터 로더
drop_last: true
pin_memory: true

# 조기 종료
early_stopping:
  enabled: true
  patience: 10
  monitor: "val_f1"
  mode: "max"

# 체크포인트
checkpoint:
  save_top_k: 3
  monitor: "val_f1"
  mode: "max"

# 로깅
log_interval: 20          # 배치가 작아졌으므로 더 자주 로깅
val_check_interval: 1.0   # 1,500장이므로 매 에포크 끝에 검증해도 충분
