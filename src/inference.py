"""
Inference ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµëœ ëª¨ë¸ë¡œ test ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  pred.csv ìƒì„±
"""

import os
import sys
import logging
import json
from pathlib import Path
from typing import Optional, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€ (ì–´ë””ì„œë“  ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import pandas as pd
import hydra
from omegaconf import DictConfig
from tqdm import tqdm

from src.data.datamodule import DocumentImageDataModule
from src.models.module import DocumentClassifierModule
from src.utils.device import get_simple_device


log = logging.getLogger(__name__)


def get_champion_checkpoint(checkpoint_dir: Path) -> Optional[Path]:
    """ì±”í”¼ì–¸ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°

    Args:
        checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬

    Returns:
        ì±”í”¼ì–¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë˜ëŠ” None
    """
    champion_dir = checkpoint_dir / "champion"
    champion_checkpoint = champion_dir / "best_model.ckpt"
    champion_info_path = champion_dir / "champion_info.json"

    if champion_checkpoint.exists():
        # ì±”í”¼ì–¸ ì •ë³´ ë¡œë“œ
        if champion_info_path.exists():
            with open(champion_info_path, 'r') as f:
                champion_info = json.load(f)

            log.info("ğŸ† ì±”í”¼ì–¸ ëª¨ë¸ ë¡œë“œ")
            log.info(f"   val_f1: {champion_info.get('val_f1', 'N/A')}")
            log.info(f"   ì›ë³¸ ê²½ë¡œ: {champion_info.get('checkpoint_path', 'N/A')}")
            log.info(f"   ì—…ë°ì´íŠ¸: {champion_info.get('updated_at', 'N/A')}")

        return champion_checkpoint

    return None


def find_best_checkpoint(checkpoint_dir: Path) -> Optional[Path]:
    """ëª¨ë“  ì‹¤í—˜ ì¤‘ ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°

    Args:
        checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬

    Returns:
        ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë˜ëŠ” None
    """
    best_checkpoint = None
    best_metric = 0.0

    # ëª¨ë“  ì‹¤í—˜ ë””ë ‰í† ë¦¬ íƒìƒ‰
    for exp_dir in checkpoint_dir.iterdir():
        if not exp_dir.is_dir() or exp_dir.name == "champion":
            continue

        # í•´ë‹¹ ì‹¤í—˜ì˜ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        ckpt_files = list(exp_dir.glob("*.ckpt"))

        for ckpt_file in ckpt_files:
            try:
                # íŒŒì¼ëª…ì—ì„œ val_f1 ì¶”ì¶œ
                # ì˜ˆ: epoch=10-val_f1=0.950.ckpt -> 0.950
                filename = ckpt_file.stem
                if 'val_f1=' in filename:
                    val_f1_str = filename.split('val_f1=')[1]
                    val_f1 = float(val_f1_str)

                    if val_f1 > best_metric:
                        best_metric = val_f1
                        best_checkpoint = ckpt_file
            except (ValueError, IndexError):
                continue

    if best_checkpoint:
        log.info(f"ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: val_f1={best_metric:.4f}")
        log.info(f"ê²½ë¡œ: {best_checkpoint}")

    return best_checkpoint


def get_test_image_ids(test_csv_path: str) -> List[str]:
    """í…ŒìŠ¤íŠ¸ CSVì—ì„œ ì´ë¯¸ì§€ ID ì¶”ì¶œ

    Args:
        test_csv_path: í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸
    """
    df = pd.read_csv(test_csv_path)
    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì´ë¯¸ì§€ íŒŒì¼ëª… ë˜ëŠ” IDë¼ê³  ê°€ì •
    return df.iloc[:, 0].tolist()


@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig) -> None:
    """ë©”ì¸ inference í•¨ìˆ˜

    Hydra configë¡œ inference ì„¤ì • ê´€ë¦¬:
        inference.checkpoint: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        inference.output: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: pred.csv)
    """
    # Hydra configì—ì„œ inference ì„¤ì • ì½ê¸°
    inference_cfg = cfg.get('inference', {})
    checkpoint_path = inference_cfg.get('checkpoint', None)
    output_path = inference_cfg.get('output', 'pred.csv')

    log.info("=" * 70)
    log.info("ğŸ”® Inference ì‹œì‘")
    log.info("=" * 70)

    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì°¾ê¸°
    if not checkpoint_path:
        checkpoint_dir = Path(cfg.checkpoint_dir)

        if not checkpoint_dir.exists():
            raise FileNotFoundError(
                f"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ '{checkpoint_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )

        # 1ìˆœìœ„: ì±”í”¼ì–¸ ëª¨ë¸
        champion_ckpt = get_champion_checkpoint(checkpoint_dir)
        if champion_ckpt:
            checkpoint_path = str(champion_ckpt)
            log.info("ì±”í”¼ì–¸ ëª¨ë¸ ì‚¬ìš© âœ“")
        else:
            # 2ìˆœìœ„: ëª¨ë“  ì‹¤í—˜ ì¤‘ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
            log.info("ì±”í”¼ì–¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ íƒìƒ‰ ì¤‘...")
            best_ckpt = find_best_checkpoint(checkpoint_dir)

            if best_ckpt:
                checkpoint_path = str(best_ckpt)
            else:
                raise FileNotFoundError(
                    f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"'{checkpoint_dir}' ë””ë ‰í† ë¦¬ì— í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"ë¨¼ì € 'python src/train.py'ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”."
                )

    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")

    log.info(f"ì‚¬ìš© ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")

    # ë°ì´í„°ëª¨ë“ˆ ìƒì„±
    test_csv_path = os.path.join(cfg.data.root_path, cfg.data.test_csv)

    if not os.path.exists(test_csv_path):
        raise FileNotFoundError(
            f"í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_csv_path}\n"
            f"ë°ì´í„°ì…‹ì„ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”."
        )

    log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_csv_path}")

    data_module = DocumentImageDataModule(
        data_root=cfg.data.root_path,
        train_csv=cfg.data.train_csv,
        test_csv=cfg.data.test_csv,
        train_image_dir=cfg.data.get('train_image_dir', 'train/'),
        test_image_dir=cfg.data.get('test_image_dir', 'test/'),
        img_size=cfg.data.img_size,
        batch_size=cfg.training.batch_size,
        num_workers=cfg.training.num_workers,
        train_val_split=cfg.data.train_val_split,
        normalization=cfg.data.normalization,
        augmentation=cfg.data.augmentation,
    )

    data_module.setup()

    # ëª¨ë¸ ë¡œë“œ
    log.info("ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = DocumentClassifierModule.load_from_checkpoint(checkpoint_path, strict=False)
    model.eval()

    # ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA -> MPS -> CPU ìë™ ê°ì§€)
    device = get_simple_device()
    model = model.to(device)
    log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # Inference ìˆ˜í–‰
    log.info("Inference ìˆ˜í–‰ ì¤‘...")
    predictions = []

    test_loader = data_module.test_dataloader()

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Predicting"):
            images, _ = batch
            images = images.to(device)

            logits = model(images)
            preds = logits.argmax(dim=1)

            predictions.extend(preds.cpu().numpy().tolist())

    log.info(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(predictions)}")

    # ê²°ê³¼ ì €ì¥
    # sample_submission.csvê°€ ìˆìœ¼ë©´ ê·¸ í˜•ì‹ ë”°ë¥´ê¸°
    sample_submission_path = os.path.join(cfg.data.root_path, "sample_submission.csv")

    if os.path.exists(sample_submission_path):
        # sample_submission.csv í˜•ì‹ìœ¼ë¡œ ì €ì¥
        log.info(f"sample_submission.csv í˜•ì‹ ì‚¬ìš©: {sample_submission_path}")
        sample_df = pd.read_csv(sample_submission_path)
        sample_df.iloc[:, 1] = predictions[:len(sample_df)]
        result_df = sample_df
    else:
        # ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (id, target)
        log.info("ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (id, target)")
        image_ids = get_test_image_ids(test_csv_path)
        result_df = pd.DataFrame({
            'id': image_ids[:len(predictions)],
            'target': predictions
        })

    # CSV ì €ì¥
    result_df.to_csv(output_path, index=False)

    log.info("=" * 70)
    log.info(f"âœ… Inference ì™„ë£Œ!")
    log.info(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {output_path}")
    log.info(f"ğŸ“Š ì˜ˆì¸¡ ìƒ˜í”Œ:")
    log.info(f"\n{result_df.head(10)}")
    log.info("=" * 70)

    # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    pred_counts = pd.Series(predictions).value_counts().sort_index()
    log.info("\nğŸ“ˆ ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬:")
    for class_id, count in pred_counts.items():
        log.info(f"  í´ë˜ìŠ¤ {class_id}: {count:4d} ({count/len(predictions)*100:5.2f}%)")


if __name__ == "__main__":
    main()
