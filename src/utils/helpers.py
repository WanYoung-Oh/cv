"""
ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
"""

import os
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, TYPE_CHECKING

import pandas as pd

if TYPE_CHECKING:
    from omegaconf import DictConfig
    from src.data.datamodule import DocumentImageDataModule

log = logging.getLogger(__name__)


def save_json(data: Dict[str, Any], save_path: str) -> None:
    """ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def load_json(path: str) -> Dict[str, Any]:
    """JSON íŒŒì¼ ë¡œë“œ"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_val_f1_from_filename(checkpoint_path: Path) -> Optional[float]:
    """
    ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…ì—ì„œ val_f1 ë©”íŠ¸ë¦­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: epoch=10-val_f1=0.950.ckpt)

    Returns:
        ì¶”ì¶œëœ val_f1 ê°’, ì‹¤íŒ¨ ì‹œ None

    Example:
        >>> extract_val_f1_from_filename(Path("epoch=10-val_f1=0.950.ckpt"))
        0.950
    """
    try:
        filename = checkpoint_path.stem
        if 'val_f1=' in filename:
            val_f1_str = filename.split('val_f1=')[1]
            return float(val_f1_str)
    except (ValueError, IndexError):
        pass
    return None


def create_datamodule_from_config(cfg: "DictConfig") -> "DocumentImageDataModule":
    """
    Hydra configì—ì„œ DocumentImageDataModuleì„ ìƒì„±í•©ë‹ˆë‹¤.

    inference/ensemble ìš©ë„ì´ë¯€ë¡œ test_csvë¡œ sample_submission_csvë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    sample_submission_csvê°€ ì—†ìœ¼ë©´ test_csvë¡œ fallbackí•©ë‹ˆë‹¤.

    Args:
        cfg: Hydra ì„¤ì • ê°ì²´ (cfg.data, cfg.training í¬í•¨)

    Returns:
        ì„¤ì •ëœ DocumentImageDataModule ì¸ìŠ¤í„´ìŠ¤
    """
    from src.data.datamodule import DocumentImageDataModule

    # inference/ensembleì—ì„œëŠ” sample_submission_csvë¥¼ test ë°ì´í„° ì†ŒìŠ¤ë¡œ ì‚¬ìš©
    test_csv = cfg.data.get('sample_submission_csv', cfg.data.get('test_csv', None))

    return DocumentImageDataModule(
        data_root=cfg.data.root_path,
        train_csv=cfg.data.train_csv,
        test_csv=test_csv,
        train_image_dir=cfg.data.get('train_image_dir', 'train/'),
        test_image_dir=cfg.data.get('test_image_dir', 'test/'),
        img_size=cfg.data.img_size,
        batch_size=cfg.training.batch_size,
        num_workers=cfg.training.num_workers,
        train_val_split=cfg.data.train_val_split,
        normalization=cfg.data.normalization,
        augmentation=cfg.data.augmentation,
        seed=cfg.get('seed', 42),
        drop_last=cfg.training.get('drop_last', False),
    )


def save_predictions_to_csv(
    predictions: List[int],
    output_path: str,
    data_root: str,
    test_csv_path: Optional[str] = None,
    task_name: str = "Inference",
) -> pd.DataFrame:
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê³  í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.

    Args:
        predictions: ì˜ˆì¸¡ ì •ìˆ˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        output_path: ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ
        data_root: ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
        test_csv_path: í…ŒìŠ¤íŠ¸ CSV ê²½ë¡œ (optional)
        task_name: ì‘ì—… ì´ë¦„ (ë¡œê¹…ìš©)

    Returns:
        ì €ì¥ëœ DataFrame
    """
    pred_values = [int(p) for p in predictions]

    # sample_submission.csvê°€ ìˆìœ¼ë©´ ê·¸ í˜•ì‹ ë”°ë¥´ê¸°
    sample_submission_path = os.path.join(data_root, "sample_submission.csv")

    if os.path.exists(sample_submission_path):
        log.info(f"sample_submission.csv í˜•ì‹ ì‚¬ìš©: {sample_submission_path}")
        sample_df = pd.read_csv(sample_submission_path)
        col_name = sample_df.columns[1]
        sample_df[col_name] = pred_values[:len(sample_df)]
        result_df = sample_df
    else:
        # ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (id, target)
        log.info("ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (id, target)")
        if test_csv_path and os.path.exists(test_csv_path):
            test_df = pd.read_csv(test_csv_path)
            image_ids = test_df.iloc[:, 0].tolist()
        else:
            image_ids = list(range(len(pred_values)))

        result_df = pd.DataFrame({
            'id': image_ids[:len(pred_values)],
            'target': pred_values
        })

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± í›„ ì €ì¥
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    result_df.to_csv(output_path, index=False)

    # ì™„ë£Œ ë¡œê¹…
    log.info("=" * 70)
    log.info(f"âœ… {task_name} ì™„ë£Œ!")
    log.info(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {output_path}")
    log.info("=" * 70)

    # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    pred_counts = pd.Series(predictions).value_counts().sort_index()
    log.info(f"\nğŸ“ˆ ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬:")
    for class_id, count in pred_counts.items():
        log.info(f"  í´ë˜ìŠ¤ {class_id}: {count:4d} ({count/len(predictions)*100:5.2f}%)")

    return result_df
