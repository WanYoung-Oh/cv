"""
Ensemble ì‹œìŠ¤í…œ
- Voting (Hard/Soft)
- Weighted Averaging
- Stacking
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€ (ì–´ë””ì„œë“  ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
import hydra
from omegaconf import DictConfig
from scipy import stats

from src.data.datamodule import DocumentImageDataModule
from src.models.module import DocumentClassifierModule
from src.utils.device import get_simple_device
from src.utils.helpers import create_datamodule_from_config, save_predictions_to_csv

log = logging.getLogger(__name__)


def load_models(checkpoint_paths: List[str]) -> List[DocumentClassifierModule]:
    """ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    models = []
    for ckpt_path in checkpoint_paths:
        if not os.path.exists(ckpt_path):
            log.warning(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_path}")
            continue

        log.info(f"ë¡œë“œ ì¤‘: {ckpt_path}")
        model = DocumentClassifierModule.load_from_checkpoint(ckpt_path)
        model.eval()
        models.append(model)

    return models


def predict_single_model(
    model: DocumentClassifierModule,
    data_loader,
    device: torch.device,
    return_probs: bool = True
) -> np.ndarray:
    """ë‹¨ì¼ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
    all_outputs = []

    with torch.no_grad():
        for batch in data_loader:
            images, _ = batch
            images = images.to(device)

            logits = model(images)

            if return_probs:
                probs = torch.softmax(logits, dim=1)
                all_outputs.append(probs.cpu().numpy())
            else:
                preds = logits.argmax(dim=1)
                all_outputs.append(preds.cpu().numpy())

    return np.concatenate(all_outputs, axis=0)


def hard_voting(predictions: List[np.ndarray]) -> np.ndarray:
    """Hard Voting: ë‹¤ìˆ˜ê²°"""
    # predictions: List[N,] -> (num_models, N)
    votes = np.stack(predictions, axis=0)
    # ê° ìƒ˜í”Œì— ëŒ€í•´ ìµœë¹ˆê°’
    final_preds, _ = stats.mode(votes, axis=0, keepdims=False)
    return final_preds


def soft_voting(probabilities: List[np.ndarray], weights: Optional[List[float]] = None) -> tuple[np.ndarray, np.ndarray]:
    """Soft Voting: í™•ë¥  í‰ê· """
    # probabilities: List[N, C] -> (num_models, N, C)
    probs = np.stack(probabilities, axis=0)

    if weights is None:
        # ê· ë“± ê°€ì¤‘ì¹˜
        avg_probs = np.mean(probs, axis=0)
    else:
        # ê°€ì¤‘ í‰ê· 
        weights = np.array(weights).reshape(-1, 1, 1)  # (num_models, 1, 1)
        avg_probs = np.sum(probs * weights, axis=0) / np.sum(weights)

    # ìµœì¢… ì˜ˆì¸¡
    final_preds = np.argmax(avg_probs, axis=1)
    return final_preds, avg_probs


def rank_averaging(probabilities: List[np.ndarray]) -> np.ndarray:
    """Rank Averaging: ìˆœìœ„ ê¸°ë°˜ ì•™ìƒë¸”"""
    # ê° ëª¨ë¸ì˜ í™•ë¥ ì„ rankë¡œ ë³€í™˜
    ranks = []
    for probs in probabilities:
        # ê° ìƒ˜í”Œì— ëŒ€í•´ í´ë˜ìŠ¤ë³„ ìˆœìœ„
        rank = np.argsort(np.argsort(-probs, axis=1), axis=1)  # ë‚®ì€ rank = ë†’ì€ í™•ë¥ 
        ranks.append(rank)

    # í‰ê·  rank
    avg_rank = np.mean(np.stack(ranks, axis=0), axis=0)

    # rankê°€ ê°€ì¥ ë‚®ì€ í´ë˜ìŠ¤ ì„ íƒ
    final_preds = np.argmin(avg_rank, axis=1)
    return final_preds


@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig) -> None:
    """ë©”ì¸ ensemble í•¨ìˆ˜

    Hydra configë¡œ ì•™ìƒë¸” ì„¤ì • ê´€ë¦¬:
        ensemble.checkpoints: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        ensemble.method: "hard_voting" | "soft_voting" | "rank_averaging"
        ensemble.weights: ëª¨ë¸ ê°€ì¤‘ì¹˜ (ì„ íƒì‚¬í•­)
        ensemble.output: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    # Hydra configì—ì„œ ì•™ìƒë¸” ì„¤ì • ì½ê¸°
    ensemble_cfg = cfg.get('ensemble', {})
    checkpoints = ensemble_cfg.get('checkpoints', [])
    method = ensemble_cfg.get('method', 'soft_voting')
    weights = ensemble_cfg.get('weights', None)

    # ì¶œë ¥ ê²½ë¡œ: datasets_fin/submission/submission_ensemble_{method}.csv
    submission_dir = os.path.join(cfg.data.root_path, "submission")
    default_output = os.path.join(submission_dir, f"submission_ensemble_{method}.csv")
    output = ensemble_cfg.get('output', default_output)

    if not checkpoints:
        raise ValueError(
            "ensemble.checkpointsê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "configs/ensemble.yamlì„ ìƒì„±í•˜ê±°ë‚˜ CLIë¡œ ì „ë‹¬í•˜ì„¸ìš”: "
            "python src/ensemble.py ensemble.checkpoints=[path1,path2]"
        )

    log.info("=" * 70)
    log.info("ğŸ”® Ensemble Inference ì‹œì‘")
    log.info("=" * 70)
    log.info(f"ë°©ë²•: {method}")
    log.info(f"ëª¨ë¸ ìˆ˜: {len(checkpoints)}")

    # ëª¨ë¸ ë¡œë“œ
    models = load_models(checkpoints)

    if len(models) == 0:
        raise ValueError("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    if len(models) == 1:
        log.warning("ëª¨ë¸ì´ 1ê°œë§Œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì•™ìƒë¸” íš¨ê³¼ ì—†ìŒ.")

    # ë°ì´í„° ë¡œë“œ (sample_submission.csvë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì†ŒìŠ¤ë¡œ ì‚¬ìš©)
    submission_csv = cfg.data.get('sample_submission_csv', cfg.data.get('test_csv', None))
    if not submission_csv:
        raise ValueError("cfg.data.sample_submission_csv ë˜ëŠ” cfg.data.test_csvê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    submission_csv_path = os.path.join(cfg.data.root_path, submission_csv)
    if not os.path.exists(submission_csv_path):
        raise FileNotFoundError(f"Submission CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {submission_csv_path}")

    # DataModule ìƒì„± (íŒ©í† ë¦¬ í•¨ìˆ˜ ì‚¬ìš©, sample_submission_csvë¥¼ test_csvë¡œ ì „ë‹¬)
    data_module = create_datamodule_from_config(cfg)
    data_module.setup()
    test_loader = data_module.test_dataloader()

    # ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA -> MPS -> CPU ìë™ ê°ì§€)
    device = get_simple_device()
    log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡
    log.info("\nğŸ“Š ëª¨ë¸ë³„ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    all_predictions = []
    all_probabilities = []

    for i, model in enumerate(models):
        model = model.to(device)
        log.info(f"\nëª¨ë¸ {i+1}/{len(models)} ì˜ˆì¸¡ ì¤‘...")

        if method == "hard_voting":
            preds = predict_single_model(model, test_loader, device, return_probs=False)
            all_predictions.append(preds)
        else:
            probs = predict_single_model(model, test_loader, device, return_probs=True)
            all_probabilities.append(probs)

    # ì•™ìƒë¸”
    log.info(f"\nğŸ”„ {method} ì ìš© ì¤‘...")

    if method == "hard_voting":
        final_preds = hard_voting(all_predictions)
        final_probs = None

    elif method == "soft_voting":
        final_preds, final_probs = soft_voting(all_probabilities, weights)

    elif method == "rank_averaging":
        final_preds = rank_averaging(all_probabilities)
        final_probs = None

    else:
        raise ValueError(
            f"Unknown ensemble method: {method}. "
            f"Supported methods: hard_voting, soft_voting, rank_averaging"
        )

    log.info(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(final_preds)}")

    # ê²°ê³¼ ì €ì¥ (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©)
    result_df = save_predictions_to_csv(
        predictions=final_preds,
        output_path=output,
        data_root=cfg.data.root_path,
        test_csv_path=submission_csv_path,
        task_name="Ensemble",
    )

    # ì•™ìƒë¸” ì •ë³´ ì €ì¥
    ensemble_info = {
        "method": method,
        "num_models": len(models),
        "checkpoints": checkpoints,
        "weights": weights,
        "output": output
    }

    info_path = output.replace(".csv", "_info.json")
    with open(info_path, 'w') as f:
        json.dump(ensemble_info, f, indent=2)

    log.info(f"\nğŸ’¾ ì•™ìƒë¸” ì •ë³´ ì €ì¥: {info_path}")


if __name__ == "__main__":
    main()
