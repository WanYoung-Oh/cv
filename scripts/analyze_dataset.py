"""
ë°ì´í„°ì…‹ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ì´ë¯¸ì§€ í¬ê¸° ë¶„í¬, í´ë˜ìŠ¤ ë¶„í¬, ë©”íƒ€ ì •ë³´
"""

from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Tuple

import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns


def get_image_info(img_path: Path) -> Tuple[int, int, float] | None:
    """ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ ë°˜í™˜ (w, h, aspect_ratio)"""
    try:
        with Image.open(img_path) as img:
            w, h = img.size
            return w, h, w / h
    except Exception:
        return None


def analyze_class_sizes(train_df: pd.DataFrame, img_dir: Path, class_names: Dict) -> Dict:
    """Classë³„ ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„

    Args:
        train_df: í•™ìŠµ ë°ì´í„° DataFrame (image_name, label)
        img_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        class_names: Class ID to name mapping

    Returns:
        Dict: Classë³„ í¬ê¸° í†µê³„ {class_id: {min/max/mean width/height}}
    """
    class_stats = {}

    print("\n[Classë³„ ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ ì¤‘...]")

    for class_id in tqdm(sorted(train_df.iloc[:, 1].unique())):
        # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
        class_df = train_df[train_df.iloc[:, 1] == class_id]
        img_paths = [img_dir / name for name in class_df.iloc[:, 0]]

        # ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘
        widths, heights = [], []
        for img_path in img_paths:
            info = get_image_info(img_path)
            if info:
                widths.append(info[0])
                heights.append(info[1])

        if widths and heights:
            class_stats[class_id] = {
                'count': len(widths),
                'min_width': min(widths),
                'max_width': max(widths),
                'mean_width': np.mean(widths),
                'min_height': min(heights),
                'max_height': max(heights),
                'mean_height': np.mean(heights),
                'class_name': class_names.get(class_id, f'Class_{class_id}')
            }

    return class_stats


def recommend_input_size(class_stats: Dict) -> None:
    """CNN vs Transformer ëª¨ë¸ì„ ìœ„í•œ ì…ë ¥ í¬ê¸° ì „ëµ ì¶”ì²œ

    Args:
        class_stats: Classë³„ í¬ê¸° í†µê³„
    """
    # ì „ì²´ ë°ì´í„° ê¸°ì¤€ í†µê³„
    all_min_width = min(s['min_width'] for s in class_stats.values())
    all_max_width = max(s['max_width'] for s in class_stats.values())
    all_min_height = min(s['min_height'] for s in class_stats.values())
    all_max_height = max(s['max_height'] for s in class_stats.values())
    all_mean_width = np.mean([s['mean_width'] for s in class_stats.values()])
    all_mean_height = np.mean([s['mean_height'] for s in class_stats.values()])

    print("\n" + "="*70)
    print("ğŸ“Š ì…ë ¥ í¬ê¸° ì „ëµ ê¶Œì¥ì‚¬í•­ (CNN vs Transformer)")
    print("="*70)

    print(f"\nì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° ë²”ìœ„:")
    print(f"  Width:  {all_min_width} ~ {all_max_width} (í‰ê· : {all_mean_width:.0f})")
    print(f"  Height: {all_min_height} ~ {all_max_height} (í‰ê· : {all_mean_height:.0f})")

    # CNN ëª¨ë¸ ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ”· CNN ëª¨ë¸ (ResNet, EfficientNet)")
    cnn_sizes = [224, 256, 384, 512]
    recommended_cnn = min(cnn_sizes, key=lambda x: abs(x - all_mean_width))
    print(f"  ê¶Œì¥ í¬ê¸°: {recommended_cnn}x{recommended_cnn}")
    print(f"  ì´ìœ : í‰ê·  í¬ê¸°({all_mean_width:.0f})ì— ê°€ì¥ ê·¼ì ‘")
    print(f"  ì˜µì…˜: {cnn_sizes}")

    # Transformer ëª¨ë¸ ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ”¶ Vision Transformer (ViT, Swin, DeiT)")
    vit_sizes = [224, 384, 512]
    recommended_vit = min(vit_sizes, key=lambda x: abs(x - all_mean_height))
    print(f"  ê¶Œì¥ í¬ê¸°: {recommended_vit}x{recommended_vit}")
    print(f"  ì´ìœ : ViTëŠ” patch ë‹¨ìœ„ ì²˜ë¦¬, 384+ í¬ê¸°ì—ì„œ ì„±ëŠ¥ ìš°ìˆ˜")
    print(f"  ì˜µì…˜: {vit_sizes}")

    # ì¶”ê°€ ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ’¡ ì¶”ê°€ ì „ëµ:")
    if all_max_width > 1000 or all_max_height > 1000:
        print(f"  âš ï¸  í° ì´ë¯¸ì§€ ì¡´ì¬ â†’ Multi-scale training ê³ ë ¤")
    if all_max_width / all_min_width > 3:
        print(f"  âš ï¸  í¬ê¸° í¸ì°¨ í¼ â†’ Adaptive pooling ë˜ëŠ” padding ì „ëµ í•„ìš”")

    aspect_ratio = all_mean_width / all_mean_height
    if aspect_ratio < 0.8 or aspect_ratio > 1.2:
        print(f"  âš ï¸  ì¢…íš¡ë¹„ ë¶ˆê· í˜• ({aspect_ratio:.2f}) â†’ ì •ì‚¬ê° resize ì‹œ ì™œê³¡ ì£¼ì˜")

    print("="*70)


def load_images_parallel(img_paths: List[Path], max_workers: int = 8) -> Tuple[List, List, List]:
    """ë³‘ë ¬ë¡œ ì´ë¯¸ì§€ ì •ë³´ ë¡œë”©"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(tqdm(executor.map(get_image_info, img_paths), total=len(img_paths)))

    widths, heights, ratios = [], [], []
    for res in results:
        if res:
            widths.append(res[0])
            heights.append(res[1])
            ratios.append(res[2])

    return widths, heights, ratios


def compute_stats(widths: List, heights: List, ratios: List) -> Dict:
    """ì´ë¯¸ì§€ í†µê³„ ê³„ì‚°"""
    return {
        "count": len(widths),
        "w_mean": np.mean(widths),
        "h_mean": np.mean(heights),
        "ar_mean": np.mean(ratios),
        "portrait": sum(r < 0.9 for r in ratios),
        "landscape": sum(r > 1.1 for r in ratios),
        "square": sum(0.9 <= r <= 1.1 for r in ratios)
    }


def plot_analysis(widths: List, heights: List, ratios: List,
                  class_counts: pd.Series, class_names: Dict, output_dir: str = ".benchmark_results"):
    """ë¶„ì„ ì‹œê°í™” ë° ì €ì¥"""
    Path(output_dir).mkdir(exist_ok=True)

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # ì´ë¯¸ì§€ í¬ê¸° ì‚°ì ë„
    axes[0].scatter(widths, heights, alpha=0.3, c='blue')
    axes[0].set_title("Image Size Distribution")
    axes[0].set_xlabel("Width")
    axes[0].set_ylabel("Height")

    # í´ë˜ìŠ¤ ë¶„í¬
    labels = [class_names.get(i, f"C{i}") for i in class_counts.index]
    sns.barplot(x=class_counts.values, y=labels, hue=labels, palette="viridis", legend=False, ax=axes[1])
    axes[1].set_title("Class Distribution")

    # ì¢…íš¡ë¹„ íˆìŠ¤í† ê·¸ë¨
    axes[2].hist(ratios, bins=30, color='green', alpha=0.7)
    axes[2].axvline(1.0, color='red', linestyle='--', label='Square')
    axes[2].set_title("Aspect Ratio Distribution")
    axes[2].set_xlabel("Ratio (W/H)")

    plt.tight_layout()
    output_path = Path(output_dir) / "dataset_analysis.png"
    plt.savefig(output_path)
    print(f"\n[ì°¨íŠ¸ ì €ì¥: {output_path}]")


def calculate_class_weights(class_counts: pd.Series) -> pd.Series:
    """í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • ê°€ì¤‘ì¹˜ ê³„ì‚°"""
    weights = 1.0 / (class_counts / class_counts.sum())
    return weights / weights.min()


def save_to_csv(data: Dict | pd.DataFrame, filename: str, output_dir: str = ".benchmark_results") -> None:
    """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    Path(output_dir).mkdir(exist_ok=True)
    output_path = Path(output_dir) / filename

    if isinstance(data, dict):
        df = pd.DataFrame([data])
    else:
        df = data

    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"[CSV ì €ì¥: {output_path}]")


def main():
    data_root = Path("datasets_fin")
    output_dir = ".benchmark_results"

    # ë©”íƒ€ ì •ë³´ ë¡œë“œ
    class_names = {}
    if (meta_path := data_root / "meta.csv").exists():
        meta_df = pd.read_csv(meta_path)
        class_names = dict(zip(meta_df.iloc[:, 0], meta_df.iloc[:, 1]))

    # í•™ìŠµ ë°ì´í„° ë¡œë“œ
    train_df = pd.read_csv(data_root / "train.csv")
    img_paths = [data_root / "train" / name for name in train_df.iloc[:, 0]]

    # ì´ë¯¸ì§€ ë¶„ì„
    print("\n[ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...]")
    widths, heights, ratios = load_images_parallel(img_paths)
    stats = compute_stats(widths, heights, ratios)

    print(f"\n[í†µê³„]")
    print(f"  í‰ê·  í¬ê¸°: {stats['w_mean']:.0f}x{stats['h_mean']:.0f} (AR: {stats['ar_mean']:.2f})")
    print(f"  ë°©í–¥ì„±: ì„¸ë¡œ {stats['portrait']}, ê°€ë¡œ {stats['landscape']}, ì •ì‚¬ê° {stats['square']}")

    # í†µê³„ë¥¼ CSVë¡œ ì €ì¥
    stats_df = pd.DataFrame([{
        'total_images': stats['count'],
        'mean_width': f"{stats['w_mean']:.0f}",
        'mean_height': f"{stats['h_mean']:.0f}",
        'mean_aspect_ratio': f"{stats['ar_mean']:.2f}",
        'portrait_count': stats['portrait'],
        'landscape_count': stats['landscape'],
        'square_count': stats['square']
    }])
    save_to_csv(stats_df, 'dataset_statistics.csv', output_dir)

    # í´ë˜ìŠ¤ ë¶„ì„
    class_counts = train_df.iloc[:, 1].value_counts().sort_index()
    weights = calculate_class_weights(class_counts)

    print("\n[í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜]")
    for idx, w in weights.items():
        print(f"  {class_names.get(idx, 'Unknown'):20s}: {w:.2f}")

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ CSVë¡œ ì €ì¥
    weights_df = pd.DataFrame([
        {
            'class_id': idx,
            'class_name': class_names.get(idx, 'Unknown'),
            'count': class_counts.get(idx, 0),
            'weight': f"{w:.2f}"
        }
        for idx, w in weights.items()
    ])
    save_to_csv(weights_df, 'class_weights.csv', output_dir)

    # ì‹œê°í™”
    plot_analysis(widths, heights, ratios, class_counts, class_names, output_dir)

    # ê¶Œì¥ì‚¬í•­
    recommendations = []
    print("\n[ê¶Œì¥ì‚¬í•­]")
    if stats['ar_mean'] < 0.8:
        rec = "ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€ê°€ ë§ìŒ â†’ Padding í¬í•¨ Resize ê¶Œì¥"
        print(f"  - {rec}")
        recommendations.append(rec)
    if (imbalance := class_counts.max() / class_counts.min()) > 2.0:
        rec = f"í´ë˜ìŠ¤ ë¶ˆê· í˜• {imbalance:.1f}ë°° â†’ Focal Loss ë˜ëŠ” ê°€ì¤‘ì¹˜ ì ìš© ê¶Œì¥"
        print(f"  - {rec}")
        recommendations.append(rec)

    # ê¶Œì¥ì‚¬í•­ì„ CSVë¡œ ì €ì¥
    if recommendations:
        rec_df = pd.DataFrame([{'recommendation': rec} for rec in recommendations])
        save_to_csv(rec_df, 'recommendations.csv', output_dir)

    # Classë³„ ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ (NEW)
    class_stats = analyze_class_sizes(train_df, data_root / "train", class_names)

    print("\n" + "="*70)
    print("ğŸ“¦ Classë³„ ì´ë¯¸ì§€ í¬ê¸° ë¶„í¬")
    print("="*70)

    # í…Œì´ë¸” í—¤ë”
    print(f"{'Class':<20s} {'Count':>6s} {'Min W':>7s} {'Max W':>7s} {'Avg W':>7s} {'Min H':>7s} {'Max H':>7s} {'Avg H':>7s}")
    print("-" * 70)

    # Classë³„ í†µê³„ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    class_stats_list = []

    # ê° Class í†µê³„ ì¶œë ¥
    for class_id in sorted(class_stats.keys()):
        s = class_stats[class_id]
        print(
            f"{s['class_name']:<20s} "
            f"{s['count']:>6d} "
            f"{s['min_width']:>7d} "
            f"{s['max_width']:>7d} "
            f"{s['mean_width']:>7.0f} "
            f"{s['min_height']:>7d} "
            f"{s['max_height']:>7d} "
            f"{s['mean_height']:>7.0f}"
        )

        # CSVìš© ë°ì´í„° ìˆ˜ì§‘
        class_stats_list.append({
            'class_id': class_id,
            'class_name': s['class_name'],
            'count': s['count'],
            'min_width': s['min_width'],
            'max_width': s['max_width'],
            'mean_width': f"{s['mean_width']:.0f}",
            'min_height': s['min_height'],
            'max_height': s['max_height'],
            'mean_height': f"{s['mean_height']:.0f}"
        })

    print("="*70)

    # Classë³„ í¬ê¸° ë¶„í¬ë¥¼ CSVë¡œ ì €ì¥
    class_stats_df = pd.DataFrame(class_stats_list)
    save_to_csv(class_stats_df, 'class_size_distribution.csv', output_dir)

    # CNN vs Transformer ì…ë ¥ í¬ê¸° ì „ëµ ê¶Œì¥
    recommend_input_size(class_stats)

    # ì…ë ¥ í¬ê¸° ì „ëµ ê¶Œì¥ì‚¬í•­ë„ CSVë¡œ ì €ì¥
    all_mean_width = np.mean([s['mean_width'] for s in class_stats.values()])
    all_mean_height = np.mean([s['mean_height'] for s in class_stats.values()])

    cnn_sizes = [224, 256, 384, 512]
    recommended_cnn = min(cnn_sizes, key=lambda x: abs(x - all_mean_width))

    vit_sizes = [224, 384, 512]
    recommended_vit = min(vit_sizes, key=lambda x: abs(x - all_mean_height))

    input_size_rec = pd.DataFrame([
        {
            'model_type': 'CNN (ResNet, EfficientNet)',
            'recommended_size': f"{recommended_cnn}x{recommended_cnn}",
            'reason': f'í‰ê·  í¬ê¸°({all_mean_width:.0f})ì— ê°€ì¥ ê·¼ì ‘',
            'options': ', '.join(map(str, cnn_sizes))
        },
        {
            'model_type': 'Transformer (ViT, Swin, DeiT)',
            'recommended_size': f"{recommended_vit}x{recommended_vit}",
            'reason': 'ViTëŠ” patch ë‹¨ìœ„ ì²˜ë¦¬, 384+ í¬ê¸°ì—ì„œ ì„±ëŠ¥ ìš°ìˆ˜',
            'options': ', '.join(map(str, vit_sizes))
        }
    ])
    save_to_csv(input_size_rec, 'input_size_recommendations.csv', output_dir)

    print(f"\nâœ… ëª¨ë“  ê²°ê³¼ê°€ '{output_dir}/' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
