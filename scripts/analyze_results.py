"""
ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- Confusion Matrix ìƒì„±
- í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
- ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ ì¶œë ¥
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import argparse
import logging
from typing import Optional

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from tqdm import tqdm

from src.models.module import DocumentClassifierModule
from src.data.datamodule import DocumentImageDataModule

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)


def load_checkpoint(checkpoint_path: str) -> DocumentClassifierModule:
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ

    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ

    Returns:
        ë¡œë“œëœ ëª¨ë¸
    """
    log.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
    model = DocumentClassifierModule.load_from_checkpoint(
        checkpoint_path,
        strict=False  # class_weights ë“± ì¶”ê°€ í‚¤ ë¬´ì‹œ
    )
    model.eval()
    return model


def get_predictions(
    model: DocumentClassifierModule,
    dataloader: torch.utils.data.DataLoader,
    device: str = "cpu"
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """ë°ì´í„°ë¡œë”ë¡œë¶€í„° ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ

    Args:
        model: ëª¨ë¸
        dataloader: ë°ì´í„°ë¡œë”
        device: ë””ë°”ì´ìŠ¤

    Returns:
        (predictions, labels, probabilities) tuple
    """
    model = model.to(device)
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="ì¶”ë¡  ì¤‘"):
            images, labels = batch
            images = images.to(device)

            # ì˜ˆì¸¡
            logits = model(images)
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1)

            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.numpy())
            all_probs.append(probs.cpu().numpy())

    predictions = np.concatenate(all_preds)
    labels = np.concatenate(all_labels)
    probabilities = np.concatenate(all_probs)

    return predictions, labels, probabilities


def plot_confusion_matrix(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    class_names: list[str],
    save_path: Optional[str] = None
):
    """Confusion Matrix ì‹œê°í™”

    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_pred: ì˜ˆì¸¡ ë ˆì´ë¸”
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í™”ë©´ì—ë§Œ í‘œì‹œ)
    """
    cm = confusion_matrix(y_true, y_pred)

    # ì •ê·œí™” (ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë„ë¡)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # í”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))

    # ì›ë³¸ confusion matrix
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names,
        ax=axes[0]
    )
    axes[0].set_title('Confusion Matrix (Counts)')
    axes[0].set_ylabel('True Label')
    axes[0].set_xlabel('Predicted Label')

    # ì •ê·œí™”ëœ confusion matrix
    sns.heatmap(
        cm_normalized,
        annot=True,
        fmt='.2f',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names,
        ax=axes[1]
    )
    axes[1].set_title('Confusion Matrix (Normalized)')
    axes[1].set_ylabel('True Label')
    axes[1].set_xlabel('Predicted Label')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        log.info(f"Confusion Matrix ì €ì¥: {save_path}")
    else:
        plt.show()

    plt.close()


def analyze_class_performance(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_probs: np.ndarray,
    class_names: list[str]
):
    """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„

    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_pred: ì˜ˆì¸¡ ë ˆì´ë¸”
        y_probs: ì˜ˆì¸¡ í™•ë¥ 
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    # Classification report
    report = classification_report(
        y_true,
        y_pred,
        target_names=class_names,
        digits=4,
        output_dict=True
    )

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df_report = pd.DataFrame(report).transpose()

    # í´ë˜ìŠ¤ë³„ í‰ê·  í™•ë¥ 
    class_probs = {}
    for i, class_name in enumerate(class_names):
        mask = y_true == i
        if mask.sum() > 0:
            avg_prob = y_probs[mask, i].mean()
            class_probs[class_name] = avg_prob

    # ê²°ê³¼ ì¶œë ¥
    log.info("\n" + "=" * 80)
    log.info("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„")
    log.info("=" * 80)

    print("\nğŸ“Š Classification Report:")
    print(df_report.to_string())

    print("\nğŸ“ˆ í´ë˜ìŠ¤ë³„ í‰ê·  í™•ë¥  (ì •ë‹µì¸ ê²½ìš°):")
    for class_name, prob in sorted(class_probs.items(), key=lambda x: x[1], reverse=True):
        print(f"  {str(class_name):50s}: {prob:.4f}")

    # ê°€ì¥ ì–´ë ¤ìš´ í´ë˜ìŠ¤ ì°¾ê¸° (F1 score ê¸°ì¤€)
    class_f1 = df_report.loc[class_names, 'f1-score'].sort_values()

    print("\nâš ï¸  ê°€ì¥ ì–´ë ¤ìš´ í´ë˜ìŠ¤ (ë‚®ì€ F1 ìˆœ):")
    for class_name, f1 in class_f1.head(5).items():
        print(f"  {str(class_name):50s}: F1 = {f1:.4f}")

    print("\nâœ… ê°€ì¥ ì‰¬ìš´ í´ë˜ìŠ¤ (ë†’ì€ F1 ìˆœ):")
    for class_name, f1 in class_f1.tail(5).items():
        print(f"  {str(class_name):50s}: F1 = {f1:.4f}")

    log.info("=" * 80)


def find_misclassified_examples(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_probs: np.ndarray,
    class_names: list[str],
    top_k: int = 10
):
    """ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ ì°¾ê¸°

    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_pred: ì˜ˆì¸¡ ë ˆì´ë¸”
        y_probs: ì˜ˆì¸¡ í™•ë¥ 
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        top_k: ì¶œë ¥í•  ê°œìˆ˜
    """
    # ì˜¤ë¶„ë¥˜ ì°¾ê¸°
    misclassified = y_true != y_pred

    if misclassified.sum() == 0:
        log.info("âœ… ì˜¤ë¶„ë¥˜ ì—†ìŒ!")
        return

    # ì˜¤ë¶„ë¥˜ ì¤‘ ê°€ì¥ í™•ì‹ í•œ ê²ƒë“¤ (ë†’ì€ í™•ë¥ ë¡œ í‹€ë¦° ê²ƒ)
    misclassified_probs = y_probs[misclassified].max(axis=1)
    misclassified_indices = np.where(misclassified)[0]

    # í™•ë¥  ê¸°ì¤€ ì •ë ¬
    sorted_indices = np.argsort(misclassified_probs)[::-1]

    log.info("\n" + "=" * 80)
    log.info(f"ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ (ì´ {misclassified.sum()}ê°œ ì¤‘ ìƒìœ„ {min(top_k, len(sorted_indices))}ê°œ)")
    log.info("=" * 80)

    for rank, idx in enumerate(sorted_indices[:top_k], 1):
        orig_idx = misclassified_indices[idx]
        true_label = y_true[orig_idx]
        pred_label = y_pred[orig_idx]
        confidence = misclassified_probs[idx]

        print(f"\n{rank}. ìƒ˜í”Œ #{orig_idx}")
        print(f"   ì‹¤ì œ: {str(class_names[true_label])}")
        print(f"   ì˜ˆì¸¡: {str(class_names[pred_label])} (í™•ë¥ : {confidence:.4f})")
        print(f"   Top-3 ì˜ˆì¸¡:")
        top3_indices = np.argsort(y_probs[orig_idx])[::-1][:3]
        for i, class_idx in enumerate(top3_indices, 1):
            print(f"     {i}. {str(class_names[class_idx]):50s}: {y_probs[orig_idx, class_idx]:.4f}")

    log.info("=" * 80)


def main():
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ê²°ê³¼ ë¶„ì„")
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--data-root",
        type=str,
        default="datasets_fin/",
        help="ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="ë°°ì¹˜ í¬ê¸°"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="analysis_results/",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="mps" if torch.backends.mps.is_available() else "cpu",
        help="ë””ë°”ì´ìŠ¤ (cpu, cuda, mps)"
    )

    args = parser.parse_args()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë¸ ë¡œë“œ
    model = load_checkpoint(args.checkpoint)

    # DataModule ìƒì„± (baseline_aug ì„¤ì • ì‚¬ìš©)
    from omegaconf import OmegaConf

    # Config ë¡œë“œ
    config_path = Path("configs/data/baseline_aug.yaml")
    if config_path.exists():
        data_config = OmegaConf.load(config_path)
    else:
        log.warning("baseline_aug.yaml ì—†ìŒ, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        data_config = OmegaConf.create({
            "img_size": 768,
            "normalization": {
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225]
            },
            "train_val_split": 0.8
        })

    data_module = DocumentImageDataModule(
        data_root=args.data_root,
        train_csv="train.csv",
        test_csv="test.csv",
        img_size=data_config.img_size,
        batch_size=args.batch_size,
        num_workers=4,
        train_val_split=data_config.get("train_val_split", 0.8),
        normalization=data_config.get("normalization"),
        augmentation=data_config.get("augmentation"),
        seed=42
    )

    data_module.setup()

    # í´ë˜ìŠ¤ ì´ë¦„
    class_names = data_module.train_dataset.classes
    log.info(f"í´ë˜ìŠ¤ ê°œìˆ˜: {len(class_names)}")

    # Validation ë°ì´í„°ë¡œ ì˜ˆì¸¡
    log.info("Validation ë°ì´í„° ë¶„ì„ ì¤‘...")
    val_dataloader = data_module.val_dataloader()
    y_pred, y_true, y_probs = get_predictions(model, val_dataloader, args.device)

    # Confusion Matrix ìƒì„±
    cm_path = output_dir / "confusion_matrix.png"
    plot_confusion_matrix(y_true, y_pred, class_names, str(cm_path))

    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
    analyze_class_performance(y_true, y_pred, y_probs, class_names)

    # ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ
    find_misclassified_examples(y_true, y_pred, y_probs, class_names, top_k=10)

    # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
    from sklearn.metrics import accuracy_score, f1_score

    accuracy = accuracy_score(y_true, y_pred)
    f1_macro = f1_score(y_true, y_pred, average='macro')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')

    log.info("\n" + "=" * 80)
    log.info("ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
    log.info("=" * 80)
    log.info(f"Accuracy:       {accuracy:.4f}")
    log.info(f"F1 (Macro):     {f1_macro:.4f}")
    log.info(f"F1 (Weighted):  {f1_weighted:.4f}")
    log.info(f"ì´ ìƒ˜í”Œ ìˆ˜:      {len(y_true)}")
    log.info(f"ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ:     {(y_true != y_pred).sum()}")
    log.info("=" * 80)

    log.info(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {output_dir}")


if __name__ == "__main__":
    main()
